{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyConfig(object):\n",
    "    '''My configuration'''\n",
    "    learning_rate = 0.001\n",
    "    grad_max = 2.0 # used for gradient clipping\n",
    "    layers_n = 2\n",
    "    use_pretrain = True\n",
    "    embed_size = 300 # if using pretrained, most have the same dimension\n",
    "    steps_n = 20 # rnn steps number\n",
    "    batch_size = 20\n",
    "    epoch_n = 2 # one epoch means training throught the dataset\n",
    "    keep_prob = 1.0 # used for dropout\n",
    "    loss_min = 0.5 # used to early stop training\n",
    "    display_iter = 5000\n",
    "    forget_bias = 1.0\n",
    "\n",
    "config = MyConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "re_patten = {'<NUM>': mycompile('^[0-9\\.,/-]+$'),\n",
    "             '<URL>': mycompile('https?://\\S+')}\n",
    "\n",
    "def norm_word(word):\n",
    "    '''normalize word'''\n",
    "    if len(word) > 0 and word[0] == '@':\n",
    "        return'<@>'\n",
    "    for key, patten in re_patten.items():\n",
    "        if patten.match(word):\n",
    "            return key\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_label(data_path):\n",
    "    '''\n",
    "    Return\n",
    "    data: list [list [word]]\n",
    "    label: np.array [num_sentence, num_words, 2]\n",
    "    '''\n",
    "    data, label = [], []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            line = line.split()\n",
    "            data.append(line[0])\n",
    "            label.append(line[1])\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finetune_words, finetune_tags = load_data_label('./data/pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "\n",
    "train_words = pd.read_csv(data_path + 'train_x.csv')['word']\n",
    "train_tags = pd.read_csv(data_path + 'train_y.csv')['tag']\n",
    "\n",
    "dev_words = pd.read_csv(data_path + 'dev_x.csv')['word']\n",
    "dev_tags = pd.read_csv(data_path + 'dev_y.csv')['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'``': 24, 'URL': 40, 'TO': 27, 'WP': 30, '.': 48, 'LS': 31, 'HT': 28, 'WDT': 14, 'RT': 38, 'NONE': 11, '$': 46, ')': 16, 'VPP': 34, 'NNP': 51, 'POS': 35, 'VB': 15, 'NN': 44, 'FW': 12, 'USR': 18, 'IN': 6, 'RP': 43, 'VBP': 25, 'NNPS': 22, 'SYM': 26, 'RBS': 23, 'JJ': 45, 'WRB': 36, 'VBZ': 1, 'UH': 41, 'PDT': 13, 'RBR': 4, '#': 47, 'CC': 50, 'VBD': 10, 'PRP$': 5, \"''\": 39, 'MD': 19, 'CD': 9, 'PRP': 42, 'JJS': 32, 'NNS': 17, ':': 53, 'WP$': 2, '<PAD>': 0, 'VBN': 37, 'TD': 52, 'VBG': 3, 'EX': 20, ',': 8, 'DT': 33, 'O': 21, '(': 49, 'RB': 7, 'JJR': 29}\n",
      "n_classes 54\n",
      "vocab_size 35781\n"
     ]
    }
   ],
   "source": [
    "'''Add padding tag with value 0 for both data and label'''\n",
    "# tags_set = set(train_tags)\n",
    "# tags_set = {'DT', 'CD', 'NNS', 'VBG', 'PRP', '(', 'IN', 'NN', 'UH', 'RBS', 'VBP', '``', 'RP', 'PRP$', 'WP$', '$', '<PAD>', 'JJS', \"''\", 'RT', 'PDT', 'NONE', 'LS', 'VBD', 'EX', 'POS', ':', 'TO', 'CC', '.', 'VBZ', ',', 'RBR', '#', 'JJ', 'WDT', 'SYM', 'URL', 'O', 'TD', 'WP', 'USR', 'VBN', 'VB', 'RB', 'HT', 'NNP', 'VPP', 'MD', ')', 'JJR', 'WRB', 'NNPS', 'FW'}\n",
    "# tags_dict = dict(zip(tags_set, range(len(tags_set))))\n",
    "# tags_dict['<PAD>'] = 0\n",
    "tags_dict = {'URL': 40, 'WP$': 2, 'IN': 6, 'VBG': 3, 'VBZ': 1, 'RBR': 4, 'PRP$': 5, 'TD': 52, 'RB': 7, 'CD': 9, 'VBD': 10, 'NONE': 11, 'WP': 30, 'FW': 12, 'PDT': 13, 'VB': 15, ')': 16, 'NNS': 17, 'USR': 18, 'MD': 19, 'EX': 20, 'O': 21, 'NNPS': 22, 'RBS': 23, ':': 53, 'WDT': 14, '``': 24, 'POS': 35, \"''\": 39, 'NNP': 51, 'TO': 27, 'HT': 28, 'JJR': 29, 'LS': 31, 'JJS': 32, 'DT': 33, 'VPP': 34, 'WRB': 36, 'VBN': 37, '<PAD>': 0, 'UH': 41, 'PRP': 42, 'RP': 43, ',': 8, 'NN': 44, 'JJ': 45, '$': 46, '#': 47, '(': 49, 'VBP': 25, 'RT': 38, 'CC': 50, 'SYM': 26, '.': 48}\n",
    "print(tags_dict)\n",
    "\n",
    "n_classes = len(tags_dict)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "inv_tags_dict = {v: k for k, v in tags_dict.items()}\n",
    "\n",
    "unique_words = set([norm_word(word) for word in train_words] + [norm_word(word) for word in finetune_words])\n",
    "unique_words.add('<UKN>')\n",
    "words_dict = dict(zip(unique_words, range(1, len(unique_words)+1)))\n",
    "words_dict['<PAD>'] = 0\n",
    "\n",
    "vocab_size = len(words_dict)\n",
    "print('vocab_size', len(words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_idx(word):\n",
    "    word = norm_word(word)\n",
    "    if word in words_dict:\n",
    "        return words_dict[word]\n",
    "    return words_dict['<UKN>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_len: 243021\n"
     ]
    }
   ],
   "source": [
    "train_data = np.asarray([word_to_idx(word) for word in train_words])\n",
    "train_label = np.asarray([tags_dict[tag] for tag in train_tags])\n",
    "\n",
    "dev_data = np.asarray([word_to_idx(word) for word in dev_words])\n",
    "dev_label = np.asarray([tags_dict[tag] for tag in dev_tags])\n",
    "dev_len = dev_data.shape[0]\n",
    "print('dev_len:', dev_len)\n",
    "\n",
    "finetune_words = np.asarray([word_to_idx(word) for word in finetune_words])\n",
    "finetune_label = np.asarray([tags_dict[tag] for tag in finetune_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_reshape(data):\n",
    "    '''Add padding and reshape data to [-1, config.steps_n]\n",
    "    data may be data or label\n",
    "    '''\n",
    "    n_instances = data.shape[0] // config.steps_n\n",
    "    if data.shape[0] % config.steps_n != 0:\n",
    "        n_instances += 1\n",
    "        data = np.concatenate((data, [0 for _ in range(n_instances * config.steps_n - data.shape[0])]), 0)\n",
    "    return np.reshape(data, (n_instances, config.steps_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (34824, 20) (34824, 20)\n",
      "Dev shape (12152, 20) (12152, 20)\n"
     ]
    }
   ],
   "source": [
    "train_data = pad_reshape(train_data)\n",
    "train_label = pad_reshape(train_label)\n",
    "dev_data = pad_reshape(dev_data)\n",
    "dev_label = pad_reshape(dev_label)\n",
    "print('Train shape', train_data.shape, train_label.shape)\n",
    "print('Dev shape', dev_data.shape, dev_label.shape)\n",
    "\n",
    "finetune_data = pad_reshape(finetune_words)\n",
    "finetune_label = pad_reshape(finetune_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_np.shape: (35781, 300)\n"
     ]
    }
   ],
   "source": [
    "if config.use_pretrain:\n",
    "    # load word2vec model pretrained on GoogleNews\n",
    "    load_big_model = False\n",
    "    if load_big_model:\n",
    "        import gensim\n",
    "        w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "        embed_np = np.zeros((vocab_size, config.embed_size), dtype=np.float32)\n",
    "        for key, val in words_dict.items():\n",
    "            if key in w2v_model: embed_np[val] = w2v_model[key]\n",
    "            else: embed_np[val] = np.zeros((config.embed_size), dtype=np.float32)\n",
    "        embed_np.dump('./my_embedding')\n",
    "    else:\n",
    "        embed_np = np.load('./my_embedding')\n",
    "    print('embed_np.shape:', embed_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # clear old graph\n",
    "\n",
    "def get_lstm_cell(size):\n",
    "    '''Get a lstm cell with size and wrapped with dropout'''\n",
    "    return tf.contrib.rnn.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(\n",
    "            size, forget_bias=config.forget_bias, state_is_tuple=True\n",
    "        ),\n",
    "        output_keep_prob=config.keep_prob\n",
    "    )\n",
    "\n",
    "def RNN(x, lstm_cell, embedding, softmax_w, softmax_b):\n",
    "    '''\n",
    "    RNN model\n",
    "    Parameters dimension transfer:\n",
    "    x[batch_size, steps_n]\n",
    "    => embedding_lookup => x_embed[batch_size, embed_size]\n",
    "    => unstack x_embed => x_embed[batch_size * [steps_n * [embed_size]]]\n",
    "    => RNN cell => outputs[batch_size * [steps_n, embed_size]]\n",
    "    => concat & reshape => outputs[batch_size * steps_n, embed_size]\n",
    "    => outputs * softmax_w + softmax_b => logits[batch_size * steps_n, n_classes]\n",
    "    '''\n",
    "    x_embed = tf.nn.embedding_lookup(embedding, x)\n",
    "    x_embed = tf.nn.dropout(x_embed, config.keep_prob)\n",
    "    x_embed = tf.unstack(x_embed, num=config.steps_n, axis=1)\n",
    "\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x_embed, dtype=tf.float32)    \n",
    "    outputs = tf.reshape(tf.concat(axis=1, values=outputs), [-1, config.embed_size])\n",
    "\n",
    "    return tf.matmul(outputs, softmax_w) + softmax_b\n",
    "\n",
    "# tf input\n",
    "x = tf.placeholder('int32', [None, config.steps_n])\n",
    "y = tf.placeholder('int32', [None, config.steps_n])\n",
    "\n",
    "# tf parameters\n",
    "# embedding[vocab_size, embed_size]\n",
    "# softmax_w[embed_size, n_classes]\n",
    "# softmax_b[n_classes]\n",
    "if config.use_pretrain:\n",
    "    embedding = tf.Variable(embed_np)\n",
    "else:\n",
    "    embedding = tf.Variable(tf.random_normal([vocab_size, config.embed_size]))\n",
    "softmax_w = tf.Variable(tf.random_normal([config.embed_size, n_classes]))\n",
    "softmax_b = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "lstm_cell = rnn.MultiRNNCell(\n",
    "    [get_lstm_cell(config.embed_size) for _ in range(config.layers_n)],\n",
    "    state_is_tuple=True\n",
    ")\n",
    "\n",
    "# Run RNN\n",
    "logits = RNN(x, lstm_cell, embedding, softmax_w, softmax_b)\n",
    "\n",
    "# Compute loss\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "    [logits],\n",
    "    [tf.reshape(y, [-1])],\n",
    "    [tf.ones([tf.size(y)])]\n",
    ")\n",
    "cost = tf.reduce_sum(loss) / config.batch_size\n",
    "\n",
    "# Gradient clipping and train\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate)\n",
    "grads = optimizer.compute_gradients(cost)\n",
    "clipped_grads = [(tf.clip_by_value(grad, -config.grad_max, config.grad_max), var) for grad, var in grads]\n",
    "train_op = optimizer.apply_gradients(clipped_grads)\n",
    "\n",
    "# Evaluate model\n",
    "pred_y = tf.to_int32(tf.argmax(logits, 1))\n",
    "correct_pred = tf.equal(pred_y, tf.reshape(y, [-1]))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, current loss: 86.23028, training accuracy: 0.04500\n",
      "Iter 5000, current loss: 6.67756, training accuracy: 0.88000\n",
      "Iter 10000, current loss: 7.87178, training accuracy: 0.87000\n",
      "Iter 15000, current loss: 3.65875, training accuracy: 0.92750\n",
      "Iter 20000, current loss: 3.87113, training accuracy: 0.91250\n",
      "Iter 25000, current loss: 2.97841, training accuracy: 0.94000\n",
      "Iter 30000, current loss: 4.14982, training accuracy: 0.93000\n",
      "Iter 0, current loss: 4.73613, training accuracy: 0.92500\n",
      "Iter 5000, current loss: 4.02439, training accuracy: 0.91750\n",
      "Iter 10000, current loss: 6.48993, training accuracy: 0.89000\n",
      "Iter 15000, current loss: 3.20460, training accuracy: 0.92500\n",
      "Iter 20000, current loss: 3.22262, training accuracy: 0.91500\n",
      "Iter 25000, current loss: 2.13039, training accuracy: 0.95000\n",
      "Iter 30000, current loss: 3.00373, training accuracy: 0.94750\n",
      "Training complete, time used: 410.21717596054077\n",
      "Dev Accuracy: 0.905365\n"
     ]
    }
   ],
   "source": [
    "'''Training is here'''\n",
    "training_iters = train_data.shape[0]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "start_t = time.time()\n",
    "for _ in range(config.epoch_n):\n",
    "    iter_i = 0\n",
    "    while iter_i < training_iters:\n",
    "        batch_x = train_data[iter_i : min(len(train_data), iter_i + config.batch_size)]\n",
    "        batch_y = train_label[iter_i : min(len(train_data), iter_i + config.batch_size)]\n",
    "        sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "        if iter_i % config.display_iter == 0:\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            print('Iter %d, current loss: %.5f, training accuracy: %.5f' % (iter_i, loss, acc))\n",
    "            if loss < config.loss_min:\n",
    "                break\n",
    "        iter_i += config.batch_size\n",
    "print('Training complete, time used:', time.time() - start_t)\n",
    "\n",
    "print('Dev Accuracy:', sess.run(accuracy, feed_dict={x: dev_data, y: dev_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FinetuneConfig(object):\n",
    "    '''My configuration'''\n",
    "    batch_size = 20\n",
    "    epoch_n = 10 # one epoch means training throught the dataset\n",
    "    display_iter = 300\n",
    "\n",
    "finetune_config = FinetuneConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, current loss: 30.93182, training accuracy: 0.69750\n",
      "Iter 300, current loss: 14.61668, training accuracy: 0.80250\n",
      "Iter 600, current loss: 7.64191, training accuracy: 0.86250\n",
      "Iter 0, current loss: 6.32613, training accuracy: 0.91000\n",
      "Iter 300, current loss: 5.17655, training accuracy: 0.92250\n",
      "Iter 600, current loss: 3.59878, training accuracy: 0.93750\n",
      "Iter 0, current loss: 3.14150, training accuracy: 0.94750\n",
      "Iter 300, current loss: 2.73355, training accuracy: 0.95750\n",
      "Iter 600, current loss: 1.82287, training accuracy: 0.97000\n",
      "Iter 0, current loss: 1.88675, training accuracy: 0.97500\n",
      "Iter 300, current loss: 1.56020, training accuracy: 0.97750\n",
      "Iter 600, current loss: 1.19642, training accuracy: 0.98000\n",
      "Iter 0, current loss: 0.89757, training accuracy: 0.98500\n",
      "Iter 300, current loss: 0.84321, training accuracy: 0.98750\n",
      "Iter 600, current loss: 0.69333, training accuracy: 0.98750\n",
      "Iter 0, current loss: 0.68134, training accuracy: 0.99750\n",
      "Iter 300, current loss: 0.61351, training accuracy: 0.99500\n",
      "Iter 600, current loss: 0.58929, training accuracy: 0.98500\n",
      "Iter 0, current loss: 0.33308, training accuracy: 0.99750\n",
      "Iter 300, current loss: 0.38134, training accuracy: 0.99500\n",
      "Iter 600, current loss: 0.52513, training accuracy: 0.99000\n",
      "Iter 0, current loss: 0.72307, training accuracy: 0.99250\n",
      "Iter 300, current loss: 0.32692, training accuracy: 0.99500\n",
      "Iter 600, current loss: 0.43110, training accuracy: 0.99250\n",
      "Iter 0, current loss: 0.40132, training accuracy: 0.99500\n",
      "Iter 300, current loss: 0.21988, training accuracy: 0.99750\n",
      "Iter 600, current loss: 0.22818, training accuracy: 0.99500\n",
      "Iter 0, current loss: 0.28103, training accuracy: 0.99750\n",
      "Iter 300, current loss: 0.20946, training accuracy: 0.99750\n",
      "Iter 600, current loss: 0.17671, training accuracy: 0.99750\n",
      "Training complete, time used: 48.13160419464111\n"
     ]
    }
   ],
   "source": [
    "'''Training is here'''\n",
    "training_iters = finetune_data.shape[0]\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(init)\n",
    "\n",
    "start_t = time.time()\n",
    "for _ in range(finetune_config.epoch_n):\n",
    "    iter_i = 0\n",
    "    while iter_i < training_iters:\n",
    "        batch_x = finetune_data[iter_i : min(len(finetune_data), iter_i + finetune_config.batch_size)]\n",
    "        batch_y = finetune_label[iter_i : min(len(finetune_data), iter_i + finetune_config.batch_size)]\n",
    "        sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "        if iter_i % finetune_config.display_iter == 0:\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            print('Iter %d, current loss: %.5f, training accuracy: %.5f' % (iter_i, loss, acc))\n",
    "        iter_i += finetune_config.batch_size\n",
    "print('Training complete, time used:', time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pos_train(data_path):\n",
    "    '''\n",
    "    Return\n",
    "    data: list [word]\n",
    "    data_len: list []\n",
    "    label: list [label]\n",
    "    '''\n",
    "    data, data_len, label = [], [], []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        data_len.append(len(lines))\n",
    "        for line in lines:\n",
    "            line = line.strip().split('\\t')\n",
    "            data.append(line[0])\n",
    "            label.append(line[1])\n",
    "    return data, data_len, label\n",
    "\n",
    "def load_pos_test(data_path):\n",
    "    '''\n",
    "    Return\n",
    "    data: list [word]\n",
    "    data_len: list []\n",
    "    '''\n",
    "    data, data_len = [], []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        data_len.append(len(lines))\n",
    "        for line in lines:\n",
    "            line = line.strip().split('\\t')\n",
    "            data.append(line[0])\n",
    "    return data, data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33360,)\n"
     ]
    }
   ],
   "source": [
    "# test_words, test_data_len, test_label = load_pos_train('./data/dev/dev.txt')\n",
    "test_words, test_data_len = load_pos_test('./data/test/test.nolabels.txt')\n",
    "test_data = np.asarray([word_to_idx(word) for word in test_words])\n",
    "test_data = pad_reshape(test_data)\n",
    "test_pred = sess.run(pred_y, feed_dict={x: test_data})\n",
    "print(test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_train(data_path, test_words, test_label, test_pred, test_data_len):\n",
    "    count = 0\n",
    "    with open(data_path, 'w') as ofile:\n",
    "        for t_len in test_data_len:\n",
    "            for _ in range(t_len):\n",
    "                ofile.write(test_words[count] + '\\t' + test_label[count] + '\\t' + inv_tags_dict[test_pred[count]] + '\\n')\n",
    "                count += 1\n",
    "            ofile.write('\\n')\n",
    "\n",
    "def write_to_test(data_path, test_words, test_pred, test_data_len):\n",
    "    count = 0\n",
    "    with open(data_path, 'w') as ofile:\n",
    "        for t_len in test_data_len:\n",
    "            for _ in range(t_len):\n",
    "                ofile.write(test_words[count] + '\\t' + inv_tags_dict[test_pred[count]] + '\\n')\n",
    "                count += 1\n",
    "            ofile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write_to_train('./data/dev/dev_pos.txt', test_words, test_label, test_pred, test_data_len)\n",
    "write_to_test('./data/test/test_pos.txt', test_words, test_pred, test_data_len)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
