{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tageval import evaluate_tagging_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyConfig(object):\n",
    "    '''My configuration'''\n",
    "    learning_rate = 0.0001\n",
    "    embed_dim = 300\n",
    "    tags_dim = 54\n",
    "    fea_dim = embed_dim + tags_dim + 2\n",
    "    hidden_dim = 256\n",
    "    class_n = 2\n",
    "    random_scale = 0.1\n",
    "    # below are related to training\n",
    "    epoch_n = 3\n",
    "    batch_size = 200\n",
    "    display_iter = 5000\n",
    "\n",
    "config = MyConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_out_path = './mlp_dev_result.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "re_patten = {'<NUM>': mycompile('^[0-9\\.,/-]+$'),\n",
    "             '<URL>': mycompile('https?://\\S+')}\n",
    "\n",
    "def norm_word(word):\n",
    "    '''normalize word'''\n",
    "    if len(word) > 0 and word[0] == '@':\n",
    "        return'<@>'\n",
    "    for key, patten in re_patten.items():\n",
    "        if patten.match(word):\n",
    "            return key\n",
    "    return word\n",
    "\n",
    "def get_words(data_path):\n",
    "    words_list = []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            words_list.append(norm_word(line.strip().split('\\t')[0]))\n",
    "    return words_list\n",
    "\n",
    "# def get_words_dict(word_list):\n",
    "#     '''get words_dict'''\n",
    "#     words_set = set(['<PAD>', '<@>', '<UKN>'])\n",
    "#     for word in word_list:\n",
    "#         words_set.add(word)\n",
    "#     words_dict = dict(zip(words_set, range(len(words_set))))\n",
    "#     return words_dict\n",
    "\n",
    "def get_words_dict(data_path):\n",
    "    '''get words_dict'''\n",
    "    words_set = set(['<PAD>', '<@>', '<UKN>'])\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            words_set.add(norm_word(line.strip().split('\\t')[0]))\n",
    "    words_dict = dict(zip(words_set, range(len(words_set))))\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 31109\n"
     ]
    }
   ],
   "source": [
    "words_dict = get_words_dict('./data/train/combined_data.txt')\n",
    "vocab_size = len(words_dict)\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_dict = {'O': 0, 'B': 1, 'I':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_np.shape: (31109, 300)\n"
     ]
    }
   ],
   "source": [
    "# load word2vec model pretrained on GoogleNews\n",
    "load_big_model = False\n",
    "if load_big_model:\n",
    "    import gensim\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    embed_np = np.zeros((vocab_size, config.embed_dim), dtype=np.float32)\n",
    "    for key, val in words_dict.items():\n",
    "        if key in w2v_model:\n",
    "            embed_np[val] = w2v_model[key]\n",
    "    embed_np.dump('./my_embedding')\n",
    "    del w2v_model\n",
    "else:\n",
    "    embed_np = np.load('./my_embedding')\n",
    "print('embed_np.shape:', embed_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': 35, 'URL': 40, 'CC': 50, 'MD': 19, 'NNPS': 22, 'VBD': 10, 'WP': 30, ',': 8, '$': 46, 'NNP': 51, '(': 49, 'RBS': 23, 'VBG': 3, 'JJ': 45, 'NONE': 11, 'RB': 7, ':': 53, ')': 16, 'O': 21, \"''\": 39, 'VBN': 37, 'UH': 41, 'WRB': 36, 'TO': 27, 'FW': 12, 'WDT': 14, 'NNS': 17, 'JJS': 32, 'JJR': 29, 'PRP$': 5, 'VPP': 34, 'PRP': 42, 'TD': 52, 'IN': 6, 'HT': 28, 'EX': 20, 'VB': 15, 'VBP': 25, 'CD': 9, 'WP$': 2, '<PAD>': 0, 'USR': 18, 'DT': 33, 'RP': 43, '.': 48, 'VBZ': 1, '``': 24, 'RBR': 4, 'PDT': 13, 'LS': 31, 'RT': 38, 'NN': 44, 'SYM': 26, '#': 47}\n"
     ]
    }
   ],
   "source": [
    "tags_dict = {'URL': 40, 'WP$': 2, 'VBG': 3, 'VBZ': 1, 'RBR': 4, 'IN': 6, 'RB': 7, 'CD': 9, 'VBD': 10, 'NONE': 11, 'JJR': 29, 'FW': 12, 'PDT': 13, 'VB': 15, ')': 16, 'NNS': 17, 'USR': 18, 'MD': 19, 'RT': 38, 'EX': 20, 'O': 21, 'NNPS': 22, 'RBS': 23, 'CC': 50, 'WDT': 14, '``': 24, 'VPP': 34, 'SYM': 26, 'NNP': 51, 'TO': 27, 'HT': 28, 'WP': 30, 'LS': 31, 'JJS': 32, 'DT': 33, 'POS': 35, 'WRB': 36, 'VBN': 37, \"''\": 39, 'UH': 41, 'PRP': 42, 'RP': 43, ',': 8, 'NN': 44, 'PRP$': 5, 'JJ': 45, '$': 46, '#': 47, '(': 49, 'VBP': 25, '<PAD>': 0, ':': 53, 'TD': 52, '.': 48}\n",
    "print(tags_dict)\n",
    "inv_tags_dict = {v: k for k, v in tags_dict.items()}\n",
    "\n",
    "tags_n = len(tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_pos_tag(tag):\n",
    "    if tag == '\\\"':\n",
    "        return tags_dict[\"''\"]\n",
    "    elif tag == 'NN|SYM':\n",
    "        return tags_dict['NN']\n",
    "    return tags_dict[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_to_idx(word):\n",
    "    word = norm_word(word)\n",
    "    if word in words_dict:\n",
    "        return words_dict[word]\n",
    "    return words_dict['<UKN>']\n",
    "\n",
    "def load_train_data(data_path):\n",
    "    '''\n",
    "    Return\n",
    "    data: list [list [word]]\n",
    "    data_idx: np.array [num_words]\n",
    "    data_pos: np.array [num_words]\n",
    "    data_cap: np.array [num_words]\n",
    "    data_len: np.array [num_sentence]\n",
    "    label: np.array [num_words, 2]\n",
    "    '''\n",
    "    data, data_idx, data_pos, data_cap, data_len, label = [], [], [], [], [], []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        sentence = sentence.strip().split('\\n')\n",
    "        for line in sentence:\n",
    "            line = line.strip().split('\\t')        \n",
    "            data.append(line[0])\n",
    "            data_idx.append(word_to_idx(line[0]))\n",
    "            data_pos.append(norm_pos_tag(line[2]))\n",
    "            data_cap.append(1 if line[0][0].isupper() else 0)\n",
    "            label.append([0, 0])\n",
    "            label[-1][label_dict[line[1]]] = 1\n",
    "        data_len.append(len(sentence))\n",
    "    return data, np.asarray(data_idx, dtype=np.int32), np.asarray(data_pos, dtype=np.int32), np.asarray(data_cap, dtype=np.int32), np.asarray(data_len, dtype=np.int32), np.asarray(label, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, train_data_idx, train_data_pos, train_data_cap, train_data_len, train_label = load_train_data('./data/train/combined_data.txt')\n",
    "dev_data, dev_data_idx, dev_data_pos, dev_data_cap, dev_data_len, dev_label = load_train_data('./data/dev/dev_pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343685,) int32\n",
      "(343685, 2) int32\n",
      "(343685,) int32\n",
      "(23072,) int32\n"
     ]
    }
   ],
   "source": [
    "print(train_data_idx.shape, train_data_idx.dtype)\n",
    "print(train_label.shape, train_label.dtype)\n",
    "print(train_data_cap.shape, train_data_cap.dtype)\n",
    "print(train_data_len.shape, train_data_len.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 300) (?, 54) (?, 2)\n",
      "(?, 356)\n",
      "(?, 256)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # clear old graph\n",
    "\n",
    "input_data = tf.placeholder(tf.int32, [None])\n",
    "input_pos = tf.placeholder(tf.int32, [None])\n",
    "input_cap = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "input_label = tf.placeholder(tf.int32, [None, config.class_n])\n",
    "\n",
    "embedding = tf.Variable(embed_np)\n",
    "softmax_w = tf.Variable(tf.random_normal(shape=[2 * config.hidden_dim, config.class_n], stddev=config.random_scale))\n",
    "softmax_b = tf.Variable(tf.random_normal(shape=[config.class_n], stddev=config.random_scale))\n",
    "\n",
    "weights = {\n",
    "    'layer_1': tf.Variable(tf.random_normal([config.fea_dim, config.hidden_dim])),\n",
    "    'layer_out': tf.Variable(tf.random_normal([config.hidden_dim, config.class_n]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'layer_1': tf.Variable(tf.random_normal([config.hidden_dim])),\n",
    "    'layer_out': tf.Variable(tf.random_normal([config.class_n]))\n",
    "}\n",
    "\n",
    "input_embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "input_pos_one_hot = tf.one_hot(input_pos, depth=tags_n, dtype=tf.float32)\n",
    "input_cap_one_hot = tf.one_hot(input_cap, depth=2, dtype=tf.float32)\n",
    "print(input_embed.get_shape(), input_pos_one_hot.get_shape(), input_cap_one_hot.get_shape())\n",
    "input_fea = tf.concat([input_embed, input_pos_one_hot, input_cap_one_hot], axis=1)\n",
    "# should be [None, config.fea_dim]\n",
    "print(input_fea.get_shape())\n",
    "\n",
    "hidden_fea = tf.matmul(input_fea, weights['layer_1']) + biases['layer_1']\n",
    "hidden_fea = tf.nn.relu(hidden_fea)\n",
    "print(hidden_fea.get_shape())\n",
    "\n",
    "output = tf.matmul(hidden_fea, weights['layer_out']) + biases['layer_out']\n",
    "pred_y = tf.to_int32(tf.argmax(output, 1))\n",
    "correct_pred = tf.equal(pred_y, input_label)\n",
    "# print(correct_pred.get_shape())\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# Compute loss\n",
    "# y_one_hot = tf.one_hot(y, depth=output_dim)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=input_label)) / config.batch_size\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=config.learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, current loss: 0.25650\n",
      "Iter 5000, current loss: 0.18363\n",
      "Iter 10000, current loss: 0.11161\n",
      "Iter 15000, current loss: 0.09278\n",
      "Iter 20000, current loss: 0.07222\n",
      "Iter 25000, current loss: 0.07948\n",
      "Iter 30000, current loss: 0.01840\n",
      "Iter 35000, current loss: 0.03125\n",
      "Iter 40000, current loss: 0.02693\n",
      "Iter 45000, current loss: 0.04486\n",
      "Iter 50000, current loss: 0.03579\n",
      "Iter 55000, current loss: 0.05911\n",
      "Iter 60000, current loss: 0.03992\n",
      "Iter 65000, current loss: 0.03166\n",
      "Iter 70000, current loss: 0.01197\n",
      "Iter 75000, current loss: 0.06403\n",
      "Iter 80000, current loss: 0.03870\n",
      "Iter 85000, current loss: 0.01569\n",
      "Iter 90000, current loss: 0.01334\n",
      "Iter 95000, current loss: 0.00899\n",
      "Iter 100000, current loss: 0.01066\n",
      "Iter 105000, current loss: 0.01798\n",
      "Iter 110000, current loss: 0.01014\n",
      "Iter 115000, current loss: 0.01636\n",
      "Iter 120000, current loss: 0.01190\n",
      "Iter 125000, current loss: 0.02199\n",
      "Iter 130000, current loss: 0.00401\n",
      "Iter 135000, current loss: 0.01573\n",
      "Iter 140000, current loss: 0.02417\n",
      "Iter 145000, current loss: 0.01887\n",
      "Iter 150000, current loss: 0.01497\n",
      "Iter 155000, current loss: 0.00716\n",
      "Iter 160000, current loss: 0.01191\n",
      "Iter 165000, current loss: 0.00581\n",
      "Iter 170000, current loss: 0.00481\n",
      "Iter 175000, current loss: 0.01149\n",
      "Iter 180000, current loss: 0.00001\n",
      "Iter 185000, current loss: 0.02620\n",
      "Iter 190000, current loss: 0.00702\n",
      "Iter 195000, current loss: 0.01987\n",
      "Iter 200000, current loss: 0.00189\n",
      "Iter 205000, current loss: 0.00373\n",
      "Iter 210000, current loss: 0.00829\n",
      "Iter 215000, current loss: 0.01918\n",
      "Iter 220000, current loss: 0.00655\n",
      "Iter 225000, current loss: 0.01808\n",
      "Iter 230000, current loss: 0.00069\n",
      "Iter 235000, current loss: 0.00376\n",
      "Iter 240000, current loss: 0.00050\n",
      "Iter 245000, current loss: 0.00206\n",
      "Iter 250000, current loss: 0.00481\n",
      "Iter 255000, current loss: 0.00767\n",
      "Iter 260000, current loss: 0.00081\n",
      "Iter 265000, current loss: 0.00108\n",
      "Iter 270000, current loss: 0.00448\n",
      "Iter 275000, current loss: 0.00192\n",
      "Iter 280000, current loss: 0.00103\n",
      "Iter 285000, current loss: 0.00281\n",
      "Iter 290000, current loss: 0.00216\n",
      "Iter 295000, current loss: 0.00294\n",
      "Iter 300000, current loss: 0.00506\n",
      "Iter 305000, current loss: 0.00814\n",
      "Iter 310000, current loss: 0.00392\n",
      "Iter 315000, current loss: 0.00026\n",
      "Iter 320000, current loss: 0.00511\n",
      "Iter 325000, current loss: 0.00502\n",
      "Iter 330000, current loss: 0.00352\n",
      "Iter 335000, current loss: 0.00507\n",
      "Iter 340000, current loss: 0.00317\n",
      "Iter 0, current loss: 0.01370\n",
      "Iter 5000, current loss: 0.01143\n",
      "Iter 10000, current loss: 0.01227\n",
      "Iter 15000, current loss: 0.02891\n",
      "Iter 20000, current loss: 0.01333\n",
      "Iter 25000, current loss: 0.01029\n",
      "Iter 30000, current loss: 0.00090\n",
      "Iter 35000, current loss: 0.00339\n",
      "Iter 40000, current loss: 0.00361\n",
      "Iter 45000, current loss: 0.02787\n",
      "Iter 50000, current loss: 0.00139\n",
      "Iter 55000, current loss: 0.00790\n",
      "Iter 60000, current loss: 0.00731\n",
      "Iter 65000, current loss: 0.00077\n",
      "Iter 70000, current loss: 0.00006\n",
      "Iter 75000, current loss: 0.00002\n",
      "Iter 80000, current loss: 0.00303\n",
      "Iter 85000, current loss: 0.00002\n",
      "Iter 90000, current loss: 0.00000\n",
      "Iter 95000, current loss: 0.00382\n",
      "Iter 100000, current loss: 0.00033\n",
      "Iter 105000, current loss: 0.00222\n",
      "Iter 110000, current loss: 0.00001\n",
      "Iter 115000, current loss: 0.00021\n",
      "Iter 120000, current loss: 0.00575\n",
      "Iter 125000, current loss: 0.00040\n",
      "Iter 130000, current loss: 0.00026\n",
      "Iter 135000, current loss: 0.00886\n",
      "Iter 140000, current loss: 0.00001\n",
      "Iter 145000, current loss: 0.00325\n",
      "Iter 150000, current loss: 0.00765\n",
      "Iter 155000, current loss: 0.00023\n",
      "Iter 160000, current loss: 0.00002\n",
      "Iter 165000, current loss: 0.00056\n",
      "Iter 170000, current loss: 0.00002\n",
      "Iter 175000, current loss: 0.00115\n",
      "Iter 180000, current loss: 0.00000\n",
      "Iter 185000, current loss: 0.00453\n",
      "Iter 190000, current loss: 0.00152\n",
      "Iter 195000, current loss: 0.00883\n",
      "Iter 200000, current loss: 0.00001\n",
      "Iter 205000, current loss: 0.00080\n",
      "Iter 210000, current loss: 0.00078\n",
      "Iter 215000, current loss: 0.01231\n",
      "Iter 220000, current loss: 0.00001\n",
      "Iter 225000, current loss: 0.00153\n",
      "Iter 230000, current loss: 0.00003\n",
      "Iter 235000, current loss: 0.00272\n",
      "Iter 240000, current loss: 0.00004\n",
      "Iter 245000, current loss: 0.00000\n",
      "Iter 250000, current loss: 0.00268\n",
      "Iter 255000, current loss: 0.00276\n",
      "Iter 260000, current loss: 0.00003\n",
      "Iter 265000, current loss: 0.00008\n",
      "Iter 270000, current loss: 0.00001\n",
      "Iter 275000, current loss: 0.00005\n",
      "Iter 280000, current loss: 0.00152\n",
      "Iter 285000, current loss: 0.00057\n",
      "Iter 290000, current loss: 0.00101\n",
      "Iter 295000, current loss: 0.00034\n",
      "Iter 300000, current loss: 0.00358\n",
      "Iter 305000, current loss: 0.00563\n",
      "Iter 310000, current loss: 0.00212\n",
      "Iter 315000, current loss: 0.00005\n",
      "Iter 320000, current loss: 0.00371\n",
      "Iter 325000, current loss: 0.00218\n",
      "Iter 330000, current loss: 0.00022\n",
      "Iter 335000, current loss: 0.00005\n",
      "Iter 340000, current loss: 0.00233\n",
      "Iter 0, current loss: 0.00391\n",
      "Iter 5000, current loss: 0.00406\n",
      "Iter 10000, current loss: 0.00851\n",
      "Iter 15000, current loss: 0.01514\n",
      "Iter 20000, current loss: 0.00856\n",
      "Iter 25000, current loss: 0.00356\n",
      "Iter 30000, current loss: 0.00031\n",
      "Iter 35000, current loss: 0.00037\n",
      "Iter 40000, current loss: 0.00173\n",
      "Iter 45000, current loss: 0.01959\n",
      "Iter 50000, current loss: 0.00023\n",
      "Iter 55000, current loss: 0.00202\n",
      "Iter 60000, current loss: 0.00367\n",
      "Iter 65000, current loss: 0.00061\n",
      "Iter 70000, current loss: 0.00008\n",
      "Iter 75000, current loss: 0.00002\n",
      "Iter 80000, current loss: 0.00001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-adb0c2ff6f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter_i\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         sess.run(train_op, feed_dict = {\n\u001b[0;32m---> 17\u001b[0;31m                 input_data: batch_x, input_pos: batch_pos, input_cap: batch_cap, input_label: batch_y})\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             tloss = sess.run(loss, feed_dict = {\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Training is here'''\n",
    "training_iters = train_data_idx.shape[0]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "start_t = time.time()\n",
    "for _ in range(config.epoch_n):\n",
    "# for _ in range(1):\n",
    "    iter_i = 0\n",
    "    while iter_i < training_iters:\n",
    "        batch_x = train_data_idx[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        batch_pos = train_data_pos[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        batch_cap = train_data_cap[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        batch_y = train_label[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        sess.run(train_op, feed_dict = {\n",
    "                input_data: batch_x, input_pos: batch_pos, input_cap: batch_cap, input_label: batch_y})\n",
    "        if iter_i % config.display_iter == 0:\n",
    "            tloss = sess.run(loss, feed_dict = {\n",
    "                    input_data: batch_x, input_pos: batch_pos, input_cap: batch_cap, input_label: batch_y})\n",
    "            print('Iter %d, current loss: %.5f' % (iter_i, tloss))\n",
    "        iter_i += config.batch_size\n",
    "print('Training complete, time used:', time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_to_file(filename, label, data_len):\n",
    "    with open(filename, 'w') as ofile:\n",
    "        count = 0\n",
    "        for sent_len in data_len:\n",
    "            for i in range(sent_len):\n",
    "                tlabel = 'O'\n",
    "                if label[count] == 1:\n",
    "                    if i > 0 and count > 0 and label[count-1] == 1:\n",
    "                        tlabel = 'I'\n",
    "                    else:\n",
    "                        tlabel = 'B'                    \n",
    "                ofile.write(tlabel+'\\n')\n",
    "                count += 1\n",
    "            ofile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev complete, time used: 0.043956756591796875\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "dev_pred = sess.run(pred_y, feed_dict = {\n",
    "        input_data: dev_data_idx, input_pos: dev_data_pos, input_cap: dev_data_cap})\n",
    "print('Dev complete, time used:', time.time() - start_t)\n",
    "label_to_file(dev_out_path, dev_pred, dev_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-level NER evaluation\n",
      "F = 0.2662,  Prec = 0.2645 (123/465),  Rec = 0.2680 (123/459)\n",
      "(959 sentences, 13360 tokens, 459 gold spans, 465 predicted spans)\n"
     ]
    }
   ],
   "source": [
    "evaluate_tagging_file('./data/dev/dev.txt', dev_out_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
