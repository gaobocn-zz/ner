{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyConfig(object):\n",
    "    '''My configuration'''\n",
    "    learning_rate = 0.0003\n",
    "    embed_dim = 300\n",
    "    hidden_dim = 256\n",
    "    step_n = 52 # max sentence length\n",
    "    layers_n = 2\n",
    "    class_n = 2\n",
    "    random_scale = 0.1\n",
    "    keep_prob = 0.5 # used for dropout\n",
    "    grad_clip = 10\n",
    "    # below are related to training\n",
    "    epoch_n = 1\n",
    "    batch_size = 20\n",
    "    display_iter = 200\n",
    "\n",
    "config = MyConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_max_len(data_path):\n",
    "    max_len = 0\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        max_len = max(max_len, len(sentence.strip().split('\\n')))\n",
    "    return max_len\n",
    "\n",
    "train_max_len = find_max_len('./data/train/train.txt')\n",
    "dev_max_len = find_max_len('./data/dev/dev.txt')\n",
    "test_max_len = find_max_len('./data/test/test.nolabels.txt')\n",
    "print(train_max_len, dev_max_len, test_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lstm_cell(size):\n",
    "    '''Get a lstm cell with size and wrapped with dropout'''\n",
    "    return tf.contrib.rnn.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(\n",
    "            size, forget_bias=config.forget_bias, state_is_tuple=True\n",
    "        ),\n",
    "        output_keep_prob=config.keep_prob\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_word(word):\n",
    "    '''normalize word'''\n",
    "    if len(word) > 0 and word[0] == '@':\n",
    "        return'<@>'\n",
    "    return word\n",
    "\n",
    "def get_words_dict(data_path):\n",
    "    '''get words_dict'''\n",
    "    words_set = set(['<PAD>', '<@>', '<UKN>'])\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            words_set.add(norm_word(line.strip().split('\\t')[0]))\n",
    "    words_dict = dict(zip(words_set, range(len(words_set))))\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 9456\n"
     ]
    }
   ],
   "source": [
    "label_dict = {'O': 0, 'B': 1, 'I':1}\n",
    "\n",
    "words_dict = get_words_dict('./data/train/train.txt')\n",
    "vocab_size = len(words_dict)\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_idx(word):\n",
    "    word = norm_word(word)\n",
    "    if word in words_dict:\n",
    "        return words_dict[word]\n",
    "    return words_dict['<UKN>']\n",
    "\n",
    "def load_data_label(data_path):\n",
    "    '''\n",
    "    Return\n",
    "    data: list [list [word]]\n",
    "    data_idx: np.array [num_sentence, num_words]\n",
    "    data_len: np.array [num_sentence]\n",
    "    label: np.array [num_sentence, num_words, 2]\n",
    "    '''\n",
    "    data, data_idx, data_len, label = [], [], [], []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        sent_data, sent_data_idx, sent_label = [], [], []\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            line = line.strip().split('\\t')\n",
    "            sent_data.append(line[0])\n",
    "            sent_data_idx.append(word_to_idx(line[0]))\n",
    "            sent_label.append([0, 0])\n",
    "            sent_label[-1][label_dict[line[1]]] = 1\n",
    "        sent_data_idx.extend([words_dict['<PAD>']] * (config.step_n - len(sent_data_idx)))\n",
    "        sent_label.extend([[0, 0]] * (config.step_n - len(sent_label)))\n",
    "        data.append(sent_data)\n",
    "        data_idx.append(sent_data_idx)\n",
    "        data_len.append(len(sent_data))\n",
    "        label.append(sent_label)\n",
    "    return data, np.asarray(data_idx), np.asarray(data_len), np.asarray(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, train_data_idx, train_data_len, train_label = load_data_label('./data/train/train.txt')\n",
    "dev_data, dev_data_idx, dev_data_len, dev_label = load_data_label('./data/dev/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2394, 52) int32\n",
      "(2394, 52, 2) int32\n"
     ]
    }
   ],
   "source": [
    "print(train_data_idx.shape, train_data_idx.dtype)\n",
    "print(train_label.shape, train_label.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: (?, 512)\n",
      "(?, 52, 2)\n",
      "(?, 52, 2)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # clear old graph\n",
    "\n",
    "input_data = tf.placeholder(tf.int32, [None, config.step_n])\n",
    "input_len = tf.placeholder(tf.int32, [None])\n",
    "input_label = tf.placeholder(tf.float32, [None, config.step_n, config.class_n])\n",
    "\n",
    "embedding = tf.Variable(tf.random_normal([vocab_size, config.embed_dim]))\n",
    "softmax_w = tf.Variable(tf.random_normal(shape=[2 * config.hidden_dim, config.class_n], stddev=config.random_scale))\n",
    "softmax_b = tf.Variable(tf.random_normal(shape=[config.class_n], stddev=config.random_scale))\n",
    "\n",
    "input_embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "# should be [None, config.step_n, config.embed_dim]\n",
    "# print(input_embed.get_shape())\n",
    "\n",
    "fw_cell = tf.contrib.rnn.LSTMCell(config.hidden_dim, state_is_tuple=True)\n",
    "fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, output_keep_prob=config.keep_prob)\n",
    "bw_cell = tf.contrib.rnn.LSTMCell(config.hidden_dim, state_is_tuple=True)\n",
    "bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob=config.keep_prob)\n",
    "fw_cell = tf.contrib.rnn.MultiRNNCell([fw_cell] * config.layers_n, state_is_tuple=True)\n",
    "bw_cell = tf.contrib.rnn.MultiRNNCell([bw_cell] * config.layers_n, state_is_tuple=True)\n",
    "# words_used_in_sent = tf.sign(tf.reduce_max(tf.abs(input_label), reduction_indices=2))\n",
    "# length = tf.cast(tf.reduce_sum(words_used_in_sent, reduction_indices=1), tf.int32)\n",
    "output, _, _ = tf.contrib.rnn.static_bidirectional_rnn(fw_cell, bw_cell,\n",
    "                                       tf.unstack(tf.transpose(input_embed, perm=[1, 0, 2])),\n",
    "                                       dtype=tf.float32, sequence_length=input_len)\n",
    "output = tf.reshape(tf.transpose(tf.stack(output), perm=[1, 0, 2]), [-1, 2 * config.hidden_dim])\n",
    "# print('output:', output.get_shape())\n",
    "logits = tf.nn.softmax(tf.matmul(output, softmax_w) + softmax_b)\n",
    "logits = tf.reshape(logits, [-1, config.step_n, config.class_n])\n",
    "\n",
    "pred_y = tf.to_int32(tf.argmax(logits, 2))\n",
    "\n",
    "# print(logits.get_shape())\n",
    "# print(input_label.get_shape())\n",
    "\n",
    "cross_entropy = input_label * tf.log(logits)\n",
    "cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "mask = tf.sign(tf.reduce_max(tf.abs(input_label), reduction_indices=2))\n",
    "cross_entropy *= mask\n",
    "cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "cross_entropy /= tf.cast(input_len, tf.float32)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(config.learning_rate)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), config.grad_clip)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, current loss: 0.55993\n",
      "Iter 200, current loss: 0.17122\n",
      "Iter 400, current loss: 0.24986\n",
      "Iter 600, current loss: 0.28127\n",
      "Iter 800, current loss: 0.22982\n",
      "Iter 1000, current loss: 0.21043\n",
      "Iter 1200, current loss: 0.30745\n",
      "Iter 1400, current loss: 0.29178\n",
      "Iter 1600, current loss: 0.23876\n",
      "Iter 1800, current loss: 0.21266\n",
      "Iter 2000, current loss: 0.24083\n",
      "Iter 2200, current loss: 0.27678\n",
      "Training complete, time used: 180.01592302322388\n"
     ]
    }
   ],
   "source": [
    "'''Training is here'''\n",
    "training_iters = train_data_idx.shape[0]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "start_t = time.time()\n",
    "for _ in range(config.epoch_n):\n",
    "    iter_i = 0\n",
    "    while iter_i < training_iters:\n",
    "        batch_x = train_data_idx[iter_i : min(len(train_data), iter_i + config.batch_size)]\n",
    "        batch_y = train_label[iter_i : min(len(train_data), iter_i + config.batch_size)]\n",
    "        batch_len = train_data_len[iter_i : min(len(train_data), iter_i + config.batch_size)]\n",
    "        sess.run(train_op, feed_dict={input_data: batch_x, input_len: batch_len, input_label: batch_y})\n",
    "        if iter_i % config.display_iter == 0:\n",
    "            tloss = sess.run(loss, feed_dict={input_data: batch_x, input_len: batch_len, input_label: batch_y})\n",
    "            print('Iter %d, current loss: %.5f' % (iter_i, tloss))\n",
    "        iter_i += config.batch_size\n",
    "print('Training complete, time used:', time.time() - start_t)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
