{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tageval import evaluate_tagging_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyConfig(object):\n",
    "    '''My configuration'''\n",
    "    learning_rate = 0.0001\n",
    "    embed_dim = 300\n",
    "    tags_dim = 54\n",
    "    fea_dim = embed_dim + tags_dim + 1\n",
    "    hidden_dim = 256\n",
    "    step_n = 52 # max sentence length\n",
    "    layers_n = 2\n",
    "    class_n = 2\n",
    "    random_scale = 0.1\n",
    "    keep_prob = 1.0 # used for dropout\n",
    "    forget_bias = 1.0\n",
    "    grad_clip = 10\n",
    "    # below are related to training\n",
    "    epoch_n = 5\n",
    "    batch_size = 10\n",
    "    display_iter = 1000\n",
    "\n",
    "config = MyConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_out_path = './dev_result.txt'\n",
    "test_out_path = './test_result.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 41 52\n"
     ]
    }
   ],
   "source": [
    "# def find_max_len(data_path):\n",
    "#     max_len = 0\n",
    "#     for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "#         max_len = max(max_len, len(sentence.strip().split('\\n')))\n",
    "#     return max_len\n",
    "\n",
    "# train_max_len = max(find_max_len('./data/train/train.txt'), find_max_len('./conll_data/conll2003_train.txt'))\n",
    "# dev_max_len = find_max_len('./data/dev/dev.txt')\n",
    "# test_max_len = find_max_len('./data/test/test.nolabels.txt')\n",
    "# print(train_max_len, dev_max_len, test_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "re_patten = {'<NUM>': mycompile('^[0-9\\.,/-]+$'),\n",
    "             '<URL>': mycompile('https?://\\S+')}\n",
    "\n",
    "def norm_word(word):\n",
    "    '''normalize word'''\n",
    "    if len(word) > 0 and word[0] == '@':\n",
    "        return'<@>'\n",
    "    for key, patten in re_patten.items():\n",
    "        if patten.match(word):\n",
    "            return key\n",
    "    return word\n",
    "\n",
    "def get_words(data_path):\n",
    "    words_list = []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            words_list.append(norm_word(line.strip().split('\\t')[0]))\n",
    "    return words_list\n",
    "\n",
    "# def get_words_dict(word_list):\n",
    "#     '''get words_dict'''\n",
    "#     words_set = set(['<PAD>', '<@>', '<UKN>'])\n",
    "#     for word in word_list:\n",
    "#         words_set.add(word)\n",
    "#     words_dict = dict(zip(words_set, range(len(words_set))))\n",
    "#     return words_dict\n",
    "\n",
    "def get_words_dict(data_path):\n",
    "    '''get words_dict'''\n",
    "    words_set = set(['<PAD>', '<@>', '<UKN>'])\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            words_set.add(norm_word(line.strip().split('\\t')[0]))\n",
    "    words_dict = dict(zip(words_set, range(len(words_set))))\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 31109\n"
     ]
    }
   ],
   "source": [
    "# words_list = get_words('./data/train/train.txt')\n",
    "# words_list.extend(get_words('./conll_data/conll2003_train.txt'))\n",
    "# words_dict = get_words_dict(words_list)\n",
    "words_dict = get_words_dict('./data/train/combined_data.txt')\n",
    "vocab_size = len(words_dict)\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_dict = {'O': 0, 'B': 1, 'I':1}\n",
    "\n",
    "# words_dict = get_words_dict('./data/train/train.txt')\n",
    "# vocab_size = len(words_dict)\n",
    "# print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_np.shape: (31109, 300)\n"
     ]
    }
   ],
   "source": [
    "# load word2vec model pretrained on GoogleNews\n",
    "load_big_model = False\n",
    "if load_big_model:\n",
    "    import gensim\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    embed_np = np.zeros((vocab_size, config.embed_dim), dtype=np.float32)\n",
    "    for key, val in words_dict.items():\n",
    "        if key in w2v_model:\n",
    "            embed_np[val] = w2v_model[key]\n",
    "    embed_np.dump('./my_embedding')\n",
    "    del w2v_model\n",
    "else:\n",
    "    embed_np = np.load('./my_embedding')\n",
    "print('embed_np.shape:', embed_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DT': 33, 'NNS': 17, 'URL': 40, ')': 16, 'WP': 30, 'UH': 41, 'FW': 12, 'VBN': 37, 'LS': 31, 'RP': 43, 'RT': 38, 'CD': 9, 'VBG': 3, ',': 8, 'IN': 6, '(': 49, '$': 46, '#': 47, 'PRP$': 5, '<PAD>': 0, 'PRP': 42, 'JJS': 32, 'MD': 19, 'VBD': 10, '.': 48, ':': 53, 'VB': 15, 'NN': 44, 'RB': 7, 'SYM': 26, 'JJ': 45, 'O': 21, 'PDT': 13, 'WP$': 2, 'NNPS': 22, 'VBP': 25, 'JJR': 29, 'POS': 35, 'EX': 20, 'VPP': 34, 'NNP': 51, 'RBR': 4, \"''\": 39, '``': 24, 'TO': 27, 'VBZ': 1, 'WRB': 36, 'USR': 18, 'RBS': 23, 'WDT': 14, 'HT': 28, 'NONE': 11, 'CC': 50, 'TD': 52}\n"
     ]
    }
   ],
   "source": [
    "tags_dict = {'URL': 40, 'WP$': 2, 'VBG': 3, 'VBZ': 1, 'RBR': 4, 'IN': 6, 'RB': 7, 'CD': 9, 'VBD': 10, 'NONE': 11, 'JJR': 29, 'FW': 12, 'PDT': 13, 'VB': 15, ')': 16, 'NNS': 17, 'USR': 18, 'MD': 19, 'RT': 38, 'EX': 20, 'O': 21, 'NNPS': 22, 'RBS': 23, 'CC': 50, 'WDT': 14, '``': 24, 'VPP': 34, 'SYM': 26, 'NNP': 51, 'TO': 27, 'HT': 28, 'WP': 30, 'LS': 31, 'JJS': 32, 'DT': 33, 'POS': 35, 'WRB': 36, 'VBN': 37, \"''\": 39, 'UH': 41, 'PRP': 42, 'RP': 43, ',': 8, 'NN': 44, 'PRP$': 5, 'JJ': 45, '$': 46, '#': 47, '(': 49, 'VBP': 25, '<PAD>': 0, ':': 53, 'TD': 52, '.': 48}\n",
    "print(tags_dict)\n",
    "inv_tags_dict = {v: k for k, v in tags_dict.items()}\n",
    "\n",
    "tags_n = len(tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_pos_tag(tag):\n",
    "    if tag == '\\\"':\n",
    "        return tags_dict[\"''\"]\n",
    "    elif tag == 'NN|SYM':\n",
    "        return tags_dict['NN']\n",
    "    return tags_dict[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_idx(word):\n",
    "    word = norm_word(word)\n",
    "    if word in words_dict:\n",
    "        return words_dict[word]\n",
    "    return words_dict['<UKN>']\n",
    "\n",
    "def load_test_data(data_path):\n",
    "    '''\n",
    "    Return\n",
    "    data: list [list [word]]\n",
    "    data_idx: np.array [num_sentence, num_words]\n",
    "    data_pos: np.array [num_sentence, num_words]\n",
    "    data_cap: np.array [num_sentence, num_words]\n",
    "    data_len: np.array [num_sentence]\n",
    "    '''\n",
    "    data, data_idx, data_pos, data_cap, data_len = [], [], [], [], []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        sent_data, sent_data_idx, sent_data_cap, sent_data_pos = [], [], [], []\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            line = line.strip().split('\\t')\n",
    "            sent_data.append(line[0])\n",
    "            sent_data_idx.append(word_to_idx(line[0]))\n",
    "            sent_data_pos.append(norm_pos_tag(line[1]))\n",
    "            sent_data_cap.append(1 if line[0][0].isupper() else 0)\n",
    "        sent_data_idx.extend([words_dict['<PAD>']] * (config.step_n - len(sent_data_idx)))\n",
    "        sent_data_pos.extend([0] * (config.step_n - len(sent_data_pos)))\n",
    "        sent_data_cap.extend([0] * (config.step_n - len(sent_data_cap)))\n",
    "        data.append(sent_data)\n",
    "        data_idx.append(sent_data_idx)\n",
    "        data_pos.append(sent_data_pos)\n",
    "        data_cap.append(sent_data_cap)\n",
    "        data_len.append(len(sent_data))\n",
    "    return data, np.asarray(data_idx, dtype=np.int32), np.asarray(data_pos, dtype=np.int32), np.asarray(data_cap, dtype=np.int32), np.asarray(data_len, dtype=np.int32)\n",
    "\n",
    "def load_train_data(data_path):\n",
    "    '''\n",
    "    Return\n",
    "    data: list [list [word]]\n",
    "    data_idx: np.array [num_sentence, num_words]\n",
    "    data_pos: np.array [num_sentence, num_words]\n",
    "    data_cap: np.array [num_sentence, num_words]\n",
    "    data_len: np.array [num_sentence]\n",
    "    label: np.array [num_sentence, num_words, 2]\n",
    "    '''\n",
    "    data, data_idx, data_pos, data_cap, data_len, label = [], [], [], [], [], []\n",
    "    for sentence in open(data_path, encoding='utf-8').read().strip().split('\\n\\n'):\n",
    "        sent_data, sent_data_idx, sent_data_cap, sent_data_pos, sent_label = [], [], [], [], []\n",
    "        for line in sentence.strip().split('\\n'):\n",
    "            line = line.strip().split('\\t')\n",
    "            sent_data.append(line[0])\n",
    "            sent_data_idx.append(word_to_idx(line[0]))\n",
    "            sent_label.append([0, 0])\n",
    "            sent_label[-1][label_dict[line[1]]] = 1\n",
    "            sent_data_pos.append(norm_pos_tag(line[2]))\n",
    "            sent_data_cap.append(1 if line[0][0].isupper() else 0)\n",
    "        sent_data_idx.extend([words_dict['<PAD>']] * (config.step_n - len(sent_data_idx)))\n",
    "        sent_data_pos.extend([0] * (config.step_n - len(sent_data_pos)))\n",
    "        sent_data_cap.extend([0] * (config.step_n - len(sent_data_cap)))\n",
    "        sent_label.extend([[0, 0]] * (config.step_n - len(sent_label)))\n",
    "        data.append(sent_data)\n",
    "        data_idx.append(sent_data_idx)\n",
    "        data_pos.append(sent_data_pos)\n",
    "        data_cap.append(sent_data_cap)\n",
    "        data_len.append(len(sent_data))\n",
    "        label.append(sent_label)\n",
    "    return data, np.asarray(data_idx, dtype=np.int32), np.asarray(data_pos, dtype=np.int32), np.asarray(data_cap, dtype=np.int32), np.asarray(data_len, dtype=np.int32), np.asarray(label, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data, train_data_idx, train_data_len = load_test_data('./data/train/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, train_data_idx, train_data_pos, train_data_cap, train_data_len, train_label = load_train_data('./data/train/combined_data.txt')\n",
    "dev_data, dev_data_idx, dev_data_pos, dev_data_cap, dev_data_len, dev_label = load_train_data('./data/dev/dev_pos.txt')\n",
    "# train_data_cap.shape = train_data_cap.shape + (1,)\n",
    "# dev_data_cap.shape = dev_data_cap.shape + (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_data_idx, test_data_pos, test_data_cap, test_data_len = load_test_data('./data/test/test_pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23072, 52) int32\n",
      "(23072, 52, 2) int32\n",
      "(23072, 52) int32\n"
     ]
    }
   ],
   "source": [
    "print(train_data_idx.shape, train_data_idx.dtype)\n",
    "print(train_label.shape, train_label.dtype)\n",
    "print(train_data_cap.shape, train_data_cap.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_data_idx, test_data_len = load_test_data('./data/test/test.nolabels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lstm_cell(size):\n",
    "    '''Get a lstm cell with size and wrapped with dropout'''\n",
    "    return tf.contrib.rnn.DropoutWrapper(\n",
    "        tf.contrib.rnn.LSTMCell(\n",
    "            size, forget_bias=config.forget_bias, state_is_tuple=True\n",
    "        ),\n",
    "        output_keep_prob=config.keep_prob\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 52, 300) (?, 52, 54) (?, 52, 2)\n",
      "(?, 52, 356)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # clear old graph\n",
    "\n",
    "input_data = tf.placeholder(tf.int32, [None, config.step_n])\n",
    "input_pos = tf.placeholder(tf.int32, [None, config.step_n])\n",
    "input_cap = tf.placeholder(tf.int32, [None, config.step_n])\n",
    "input_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "input_label = tf.placeholder(tf.float32, [None, config.step_n, config.class_n])\n",
    "\n",
    "embedding = tf.Variable(embed_np)\n",
    "softmax_w = tf.Variable(tf.random_normal(shape=[2 * config.hidden_dim, config.class_n], stddev=config.random_scale))\n",
    "softmax_b = tf.Variable(tf.random_normal(shape=[config.class_n], stddev=config.random_scale))\n",
    "\n",
    "input_embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "input_pos_one_hot = tf.one_hot(input_pos, depth=tags_n, dtype=tf.float32)\n",
    "input_cap_one_hot = tf.one_hot(input_cap, depth=2, dtype=tf.float32)\n",
    "print(input_embed.get_shape(), input_pos_one_hot.get_shape(), input_cap_one_hot.get_shape())\n",
    "input_fea = tf.concat([input_embed, input_pos_one_hot, input_cap_one_hot], axis=2)\n",
    "# should be [None, config.step_n, config.fea_dim]\n",
    "print(input_fea.get_shape())\n",
    "\n",
    "fw_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(config.hidden_dim)] * config.layers_n, state_is_tuple=True)\n",
    "bw_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(config.hidden_dim)] * config.layers_n, state_is_tuple=True)\n",
    "output, _, _ = tf.contrib.rnn.static_bidirectional_rnn(fw_cell, bw_cell,\n",
    "                                       tf.unstack(tf.transpose(input_fea, perm=[1, 0, 2])),\n",
    "                                       dtype=tf.float32, sequence_length=input_len)\n",
    "output = tf.reshape(tf.transpose(tf.stack(output), perm=[1, 0, 2]), [-1, 2 * config.hidden_dim])\n",
    "# print('output:', output.get_shape())\n",
    "logits = tf.nn.softmax(tf.matmul(output, softmax_w) + softmax_b)\n",
    "logits = tf.reshape(logits, [-1, config.step_n, config.class_n])\n",
    "\n",
    "pred_y = tf.to_int32(tf.argmax(logits, 2))\n",
    "\n",
    "# print(logits.get_shape())\n",
    "# print(input_label.get_shape())\n",
    "\n",
    "cross_entropy = input_label * tf.log(logits)\n",
    "cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "mask = tf.sign(tf.reduce_max(tf.abs(input_label), reduction_indices=2))\n",
    "cross_entropy *= mask\n",
    "cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "cross_entropy /= tf.cast(input_len, tf.float32)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(config.learning_rate)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), config.grad_clip)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32 int32 int32\n"
     ]
    }
   ],
   "source": [
    "print(train_data_idx.dtype, train_data_pos.dtype, train_data_cap.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, current loss: 0.80071\n",
      "Iter 1000, current loss: 0.25278\n",
      "Iter 2000, current loss: 0.19532\n",
      "Iter 3000, current loss: 0.51025\n",
      "Iter 4000, current loss: 0.11887\n",
      "Iter 5000, current loss: 0.14929\n",
      "Iter 6000, current loss: 0.18773\n",
      "Iter 7000, current loss: 0.16350\n",
      "Iter 8000, current loss: 0.04540\n",
      "Iter 9000, current loss: 0.01795\n",
      "Iter 10000, current loss: 0.02717\n",
      "Iter 11000, current loss: 0.00900\n",
      "Iter 12000, current loss: 0.04182\n",
      "Iter 13000, current loss: 0.01675\n",
      "Iter 14000, current loss: 0.01248\n",
      "Iter 15000, current loss: 0.06180\n",
      "Iter 16000, current loss: 0.00891\n",
      "Iter 17000, current loss: 0.01173\n",
      "Iter 18000, current loss: 0.01015\n",
      "Iter 19000, current loss: 0.03032\n",
      "Iter 20000, current loss: 0.36667\n",
      "Iter 21000, current loss: 0.01943\n",
      "Iter 22000, current loss: 0.07754\n",
      "Iter 23000, current loss: 0.03351\n",
      "Iter 0, current loss: 0.32233\n",
      "Iter 1000, current loss: 0.12696\n",
      "Iter 2000, current loss: 0.04874\n",
      "Iter 3000, current loss: 0.02487\n",
      "Iter 4000, current loss: 0.03145\n",
      "Iter 5000, current loss: 0.06958\n",
      "Iter 6000, current loss: 0.04396\n",
      "Iter 7000, current loss: 0.05394\n",
      "Iter 8000, current loss: 0.01236\n",
      "Iter 9000, current loss: 0.00539\n",
      "Iter 10000, current loss: 0.01667\n",
      "Iter 11000, current loss: 0.00249\n",
      "Iter 12000, current loss: 0.03404\n",
      "Iter 13000, current loss: 0.00842\n",
      "Iter 14000, current loss: 0.01013\n",
      "Iter 15000, current loss: 0.03877\n",
      "Iter 16000, current loss: 0.00356\n",
      "Iter 17000, current loss: 0.00499\n",
      "Iter 18000, current loss: 0.00240\n",
      "Iter 19000, current loss: 0.02286\n",
      "Iter 20000, current loss: 0.23369\n",
      "Iter 21000, current loss: 0.02050\n",
      "Iter 22000, current loss: 0.03656\n",
      "Iter 23000, current loss: 0.02449\n",
      "Iter 0, current loss: 0.25024\n",
      "Iter 1000, current loss: 0.09600\n",
      "Iter 2000, current loss: 0.03340\n",
      "Iter 3000, current loss: 0.01472\n",
      "Iter 4000, current loss: 0.02671\n",
      "Iter 5000, current loss: 0.02525\n",
      "Iter 6000, current loss: 0.04896\n",
      "Iter 7000, current loss: 0.02908\n",
      "Iter 8000, current loss: 0.00758\n",
      "Iter 9000, current loss: 0.00328\n",
      "Iter 10000, current loss: 0.00915\n",
      "Iter 11000, current loss: 0.00203\n",
      "Iter 12000, current loss: 0.03305\n",
      "Iter 13000, current loss: 0.00297\n",
      "Iter 14000, current loss: 0.00733\n",
      "Iter 15000, current loss: 0.01871\n",
      "Iter 16000, current loss: 0.00246\n",
      "Iter 17000, current loss: 0.00247\n",
      "Iter 18000, current loss: 0.00070\n",
      "Iter 19000, current loss: 0.01145\n",
      "Iter 20000, current loss: 0.15018\n",
      "Iter 21000, current loss: 0.01959\n",
      "Iter 22000, current loss: 0.01311\n",
      "Iter 23000, current loss: 0.01590\n",
      "Iter 0, current loss: 0.16490\n",
      "Iter 1000, current loss: 0.05557\n",
      "Iter 2000, current loss: 0.02426\n",
      "Iter 3000, current loss: 0.00876\n",
      "Iter 4000, current loss: 0.01782\n",
      "Iter 5000, current loss: 0.01006\n",
      "Iter 6000, current loss: 0.05856\n",
      "Iter 7000, current loss: 0.01874\n",
      "Iter 8000, current loss: 0.00537\n",
      "Iter 9000, current loss: 0.00221\n",
      "Iter 10000, current loss: 0.00425\n",
      "Iter 11000, current loss: 0.00200\n",
      "Iter 12000, current loss: 0.02237\n",
      "Iter 13000, current loss: 0.00131\n",
      "Iter 14000, current loss: 0.00286\n",
      "Iter 15000, current loss: 0.00702\n",
      "Iter 16000, current loss: 0.00104\n",
      "Iter 17000, current loss: 0.00142\n",
      "Iter 18000, current loss: 0.00022\n",
      "Iter 19000, current loss: 0.00244\n",
      "Iter 20000, current loss: 0.05885\n",
      "Iter 21000, current loss: 0.01519\n",
      "Iter 22000, current loss: 0.00587\n",
      "Iter 23000, current loss: 0.00692\n",
      "Iter 0, current loss: 0.07848\n",
      "Iter 1000, current loss: 0.02688\n",
      "Iter 2000, current loss: 0.01885\n",
      "Iter 3000, current loss: 0.00384\n",
      "Iter 4000, current loss: 0.00520\n",
      "Iter 5000, current loss: 0.00433\n",
      "Iter 6000, current loss: 0.07819\n",
      "Iter 7000, current loss: 0.01074\n",
      "Iter 8000, current loss: 0.00253\n",
      "Iter 9000, current loss: 0.00112\n",
      "Iter 10000, current loss: 0.00203\n",
      "Iter 11000, current loss: 0.00157\n",
      "Iter 12000, current loss: 0.00920\n",
      "Iter 13000, current loss: 0.00037\n",
      "Iter 14000, current loss: 0.00069\n",
      "Iter 15000, current loss: 0.00197\n",
      "Iter 16000, current loss: 0.00024\n",
      "Iter 17000, current loss: 0.00069\n",
      "Iter 18000, current loss: 0.00009\n",
      "Iter 19000, current loss: 0.00028\n",
      "Iter 20000, current loss: 0.00925\n",
      "Iter 21000, current loss: 0.00681\n",
      "Iter 22000, current loss: 0.00335\n",
      "Iter 23000, current loss: 0.00283\n",
      "Training complete, time used: 2570.906982898712\n"
     ]
    }
   ],
   "source": [
    "'''Training is here'''\n",
    "training_iters = train_data_idx.shape[0]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "start_t = time.time()\n",
    "for _ in range(config.epoch_n):\n",
    "    iter_i = 0\n",
    "    while iter_i < training_iters:\n",
    "        batch_x = train_data_idx[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        batch_pos = train_data_pos[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        batch_cap = train_data_cap[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        batch_len = train_data_len[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        batch_y = train_label[iter_i : min(len(train_data_idx), iter_i + config.batch_size)]\n",
    "        sess.run(train_op, feed_dict = {\n",
    "                input_data: batch_x, input_pos: batch_pos, input_cap: batch_cap,\n",
    "                input_len: batch_len, input_label: batch_y})\n",
    "        if iter_i % config.display_iter == 0:\n",
    "            tloss = sess.run(loss, feed_dict = {\n",
    "                    input_data: batch_x, input_pos: batch_pos, input_cap: batch_cap,\n",
    "                    input_len: batch_len, input_label: batch_y})\n",
    "            print('Iter %d, current loss: %.5f' % (iter_i, tloss))\n",
    "        iter_i += config.batch_size\n",
    "print('Training complete, time used:', time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/ner_model1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './models/ner_model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_to_file(filename, label, data_len):\n",
    "    with open(filename, 'w') as ofile:\n",
    "        for sent_label, sent_len in zip(label, data_len):\n",
    "            for i in range(sent_len):\n",
    "                tlabel = 'O'\n",
    "                if sent_label[i] == 1:\n",
    "                    if i > 0 and sent_label[i-1] == 1:\n",
    "                        tlabel = 'I'\n",
    "                    else:\n",
    "                        tlabel = 'B'                    \n",
    "                ofile.write(tlabel+'\\n')\n",
    "            ofile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_pred = sess.run(pred_y, feed_dict = {\n",
    "        input_data: dev_data_idx, input_pos: dev_data_pos, input_cap: dev_data_cap,\n",
    "        input_len: dev_data_len})\n",
    "label_to_file(dev_out_path, dev_pred, dev_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-level NER evaluation\n",
      "F = 0.4562,  Prec = 0.4841 (198/409),  Rec = 0.4314 (198/459)\n",
      "(959 sentences, 13360 tokens, 459 gold spans, 409 predicted spans)\n",
      "Test complete, time used: 0.0693213939666748\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "evaluate_tagging_file('./data/dev/dev.txt', dev_out_path)\n",
    "print('Test complete, time used:', time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_pred = sess.run(pred_y, feed_dict = {\n",
    "        input_data: test_data_idx, input_pos: test_data_pos, input_cap: test_data_cap,\n",
    "        input_len: test_data_len})\n",
    "label_to_file(test_out_path, test_pred, test_data_len)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
